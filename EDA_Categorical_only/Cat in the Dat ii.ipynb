{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas  as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import category_encoders as ce\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "train_df['type'] = 'train'\n",
    "test_df['type'] = 'test'\n",
    "all_data = pd.concat([train_df, test_df],axis = 0)\n",
    "print(len(train_df))\n",
    "print(len(test_df))\n",
    "print(len(all_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Missing Value Imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = []\n",
    "cat_vars = []\n",
    "for col in train_df.columns:\n",
    "    if train_df[col].dtypes == 'O':\n",
    "        cat_vars.append(col)\n",
    "    else:\n",
    "        num_vars.append(col)\n",
    "print(len(num_vars))\n",
    "print(len(cat_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars.remove('id')\n",
    "num_vars.remove('target')\n",
    "num_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in all_data.columns:\n",
    "    print(col, 'has', all_data[col].isnull().sum(), 'null values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_vars = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']\n",
    "ord_vars = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']\n",
    "nom_vars = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in bin_vars:\n",
    "    print(train_df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[['bin_0', 'bin_1', 'bin_2']] = all_data[['bin_0', 'bin_1', 'bin_2']].fillna(0.0)\n",
    "all_data['bin_3'].fillna('F', inplace = True)\n",
    "all_data['bin_4'].fillna('N', inplace = True)\n",
    "for col in all_data.columns:\n",
    "    print(col, 'has', all_data[col].isnull().sum(), 'null values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ord_vars:\n",
    "    print(train_df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[ord_vars] = all_data[ord_vars].replace(np.nan, 'missing', regex = True)\n",
    "for col in all_data.columns:\n",
    "    print(col, 'has', all_data[col].isnull().sum(), 'null values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in nom_vars:\n",
    "    print(train_df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['nom_0'].fillna('Red', inplace = True)\n",
    "all_data['nom_4'].fillna(\"Theremin\", inplace = True)\n",
    "all_data[['nom_1', 'nom_2', 'nom_3', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']] = all_data[['nom_1', 'nom_2', 'nom_3', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']].fillna('missing')\n",
    "for col in all_data.columns:\n",
    "    print(col, 'has', all_data[col].isnull().sum(), 'null values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[['day', 'month']] = all_data[['day', 'month']].fillna(0.0)\n",
    "for col in all_data.columns:\n",
    "    print(col, 'has', all_data[col].isnull().sum(), 'null values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[bin_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['bin_3'] = [0 if i == 'F' else 1 for i in all_data['bin_3']]\n",
    "all_data['bin_4'] = [0 if i == 'N' else 1 for i in all_data['bin_4']]\n",
    "all_data[bin_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = LabelEncoder()\n",
    "for col in ord_vars:\n",
    "    all_data[col] = lbl_enc.fit_transform(all_data[col].astype('str').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_card_nom_vars = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4']\n",
    "high_card_nom_vars = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_var_dummies = pd.get_dummies(all_data[low_card_nom_vars], drop_first = True)\n",
    "new_data = pd.concat([all_data, nom_var_dummies], axis = 1)\n",
    "print(all_data.shape)\n",
    "print(new_data.shape)\n",
    "new_data.drop(low_card_nom_vars, axis = 1, inplace = True)\n",
    "print(new_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashEncoder = ce.HashingEncoder(cols = high_card_nom_vars)\n",
    "new_data = hashEncoder.fit_transform(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = new_data[new_data['type'] == 'train']\n",
    "train_target = train_set['target']\n",
    "train_set.drop(['id', 'target', 'type'], axis = 1, inplace = True)\n",
    "test_set = new_data[new_data['type'] == 'test']\n",
    "test_set.drop('type', axis = 1, inplace = True)\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "new_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stds = new_data.describe().loc['std'].tolist()\n",
    "variances = [i**2 for i in stds]\n",
    "variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "sns.distplot(variances, bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Threshold checks the feature variances and removes which are below threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_th = VarianceThreshold(threshold = 0)\n",
    "var_th.fit_transform(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_th.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_columns = [col for col in train_set.columns if col not in train_set.columns[var_th.get_support()]]\n",
    "constant_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.columns[var_th.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal of Quasi-Constant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quasi - Constant features are features which are almost constant. These features have same value for a very large subset of values in a particular feature.  There is no rule as to what should be the threshold for the variance of quasi-constant features. However, as a rule of thumb, remove those quasi-constant features that have more than 99% similar values for the output observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quasi_const = VarianceThreshold(threshold=0.01)\n",
    "quasi_const.fit(train_set)\n",
    "quasi_const.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove duplicate variables which have the sae value as they do not contribute to the model instead act as a noise and hinder model performance. We do not have a method to remove duplicate columns but we have pd.duplicated() which removes duplicate rows. We can transpose the df and use this method. However, this is computationally very expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_T = train_set.T\n",
    "train_set_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set_T = train_set_T[~train_set_T.duplicated(keep = 'first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_set.corr()\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.heatmap(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get features with correlation greater than threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rather than any method, just try to figure out manually, where the correlation is greater than threshold(to be decided manually). For any two features found, check for the one with lower variance and eliminate that one.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting features on the basis of ROC- AUC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with roc_auc score > 0.5 are considered as good and those with the score less than 0.5 can be discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = []\n",
    "rfc = RandomForestClassifier(class_weight='balanced')\n",
    "for col in train_set.columns:\n",
    "    rfc.fit(train_set[[col]], train_target)\n",
    "    y_pred = rfc.predict(train_set[[col]])\n",
    "    roc_auc.append(roc_auc_score(train_target, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the result, all are > 0.5, so we won't remove any feature\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will make a copy of datasts because we need to scale the features for modelling, and random forest, decision tree\n",
    "# does not require any scaling\n",
    "train_set_copy = train_set.copy()\n",
    "test_set_copy = test_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_sclr = StandardScaler()\n",
    "train_set_copy = std_sclr.fit_transform(train_set_copy)\n",
    "test_set_copy = std_sclr.fit_transform(test_set_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_set_copy, train_target, test_size = 0.2, random_state = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a dataframe to save model's performance\n",
    "model_df = pd.DataFrame(columns = [\"model_name\",\"training roc_auc\",\"test_roc_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GLM Model\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "logm1 = sm.GLM(y_train, X_train_sm, class_weight = 'balanced', family = sm.families.Binomial())\n",
    "logm1.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = cols.drop('bin_3')\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_sm = sm.add_constant(X_train[cols])\n",
    "logm2 = sm.GLM(y_train, X_train_sm, class_weight = 'balanced', family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = cols.drop('nom_1_missing')\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sm = sm.add_constant(X_train[cols])\n",
    "logm3 = sm.GLM(y_train, X_train_sm, class_weight = 'balanced', family = sm.families.Binomial())\n",
    "res = logm3.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "vif['Features'] = cols\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[cols].values, i) for i in range(len(cols))]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values('VIF', ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since nom_3_india has a high vif, we will remove this column\n",
    "cols = cols.drop(['nom_3_India', 'nom_3_Costa Rica'])\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = cols\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[cols].values, i) for i in range(len(cols))]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values('VIF', ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[cols]\n",
    "X_test = X_test[cols]\n",
    "\n",
    "lgr = LogisticRegression(class_weight='balanced')\n",
    "lgr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
